### 주성분 분석(PCA, Principal Component Analysis)

**관련 개념**
- 투영행렬(Projection matrix)
어떤 벡터를 다른 어떤 공간으로 투영시키는 것

**PCA란?**
데이터 내에서 최대 분산 방향을 식별하는 것.

어떤 방향으로의 투영에서 분산이 크다는 것은, **그 방향으로 데이터들이 많이 퍼져 있다**는 의미.
즉, PCA는 데이터의 정보를 최대한 보존하는 방향을 찾고 그 방향을 따라 데이터를 새롭게 표현하는 것
이를 통해 정보 손실을 최소화하고 노이즈를 줄여 데이터 분석의 효율성을 높일 수 있다.

**PCA의 단계**
1. $d$ 차원 데이터셋을 표준화 전처리한다.
2. 공분산 행렬(covariance matrix)을 만든다.
3. 공분산 행렬을 고유 벡터와 고윳값으로 분해한다.
4. 고윳값을 내림차순으로 정렬하고 그에 해당하는 고유 벡터의 순위를 매긴다.
5. 고윳값이 가장 큰 $k$ 개의 고유 벡터를 선택한다. 여기에서 $k$ 는 새로운 특성 부분 공간의 차원이다. ($k \le d$)
6. 최상위 $k$ 개의 고유 벡터로 투영 행렬 $\mathbf{W}$ 를 만든다.
7. 투영 행렬 $\mathbf{W}$ 를 사용해어 $d$ 차원 입력 데이터셋 $\mathbf{X}$ 를 새로운 $k$ 차원의 특성 부분 공간으로 변환한다.

**공분산 계산**
전체 샘플에 대한 두 특성 $x_j$와 $x_k$ 사이의 공분산은 다음 식으로 계산할 수 있다.

$$
\sigma_{jk} = \frac {1}{n-1}
\sum^n_{i=1}
(x^{(i)}_j - \micro_j) (x^{(i)}_k - \micro_k)
$$

여기에서 $\micro_j$와 $\micro_k$는 특성 $j$ 와 $k$ 의 샘플 평균이다.
데이터셋을 표준화 전처리하면 샘플 평균은 0이다.

- **특성 기여도 평가**

**로딩(loadings)** : 원본 특성이 주성분에 얼마나 기여하는지의 정도

로딩은 **고유 벡터에 고윳값의 제곱근을 곱해** 계산할 수 있다.

```python
sklearn_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
```

---
### 선형 판별 분석 (LDA, Linear Discriminant Analysis)

**PCA**가 데이터셋에 있는 **분산이 최대인 직교 성분 축**을 찾으려고 하는 반면,
**LDA**의 목표는 클래스를 최적으로 구분할 수 있는 **특성 부분 공간**을 찾는 것이다.

PCA와 LDA 모두 데이터셋의 차원 개수를 줄일 수 있는 선형 변환 기법이다.
**PCA**는 **비지도 학습 알고리즘**이고 **LDA**는 **지도 학습 알고리즘**이다.

**LDA의 단계**
1. $d$ 차원의 데이터셋에 표준화 전처리한다. ($d$ 는 특성 개수)
2. 각 클래스에 대해 $d$ 차원의 평균 벡터를 계산한다.
3. 클래스 간의 산포 행렬(scatter matrix) $\mathbf{S}_B$와 클래스 내 산포 행렬 $\mathbf{S}_W$를 구성한다.
4. $\mathbf{S}_W^{-1} \mathbf{S}_B$ 행렬의 고유 벡터와 고윳값을 계산한다.
5. 고윳값을 내림차순으로 정렬하여 고유 벡터의 순서를 매긴다.
6. 고윳값이 가장 큰 $k$개의 고유 벡터를 선택하여 $d \times k$ 차원의 변환 행렬 $\mathbf{W}$를 구성한다. (이 행렬의 열이 고유 벡터이다.)
7. 변환 행렬 $\mathbf{W}$를 사용하여 샘플을 새로운 특성 부분 공간으로 투영한다.

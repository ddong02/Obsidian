클래스 불균형은 모델이 훈련되는 동안 학습 알고리즘 자체에 영향을 미친다.

훈련 과정에서 비용을 최소화하거나 보상을 최대화하기 위해 데이터셋에서 가장 빈도가 높은 클래스의 예측을 최적화하는 모델을 학습한다.

모델을 훈련하는 동안 불균형한 클래스를 다루는 한 가지 방법은 소수 클래스에서 발생한 예측 오류에 큰 벌칙을 부여하는 것이다.
사이킷런에서는 `class_weight` 매개변수를 `class_weight='balanced'` 로 설정해서 조정할 수 있다.

또 다른 전략은 소수 클래스의 샘플을 늘리거나 다수 클래스의 샘플을 줄이거나 인공적으로 훈련 샘플을 생성하는 것이다. → 사이킷런의 `resample` 함수

**오버샘플링의 예시** → 샘플 수 늘리기

```python
X_upsampled, y_upsampled = resample(X_imb[y_imb == 1],
                                    y_imb[y_imb == 1],
                                    replace=True,
                                    n_samples=X_imb[y_imb == 0].shape[0],
                                    random_state=123)
```

- `X_imb[y_imb == 1]` 오버샘플링 할 데이터의 특징 (클래스 1)
- `y_imb[y_imb == 1]` 오버샘플링 할 데이터의 레이블 (클래스 1)
- `replace=True` 복원 추출 (한 번 뽑힌 샘플이 다시 뽑힐 수 있도록 허용)
- `n_samples=X_imb[y_imb == 0].shape[0]` 오버샘플링 후 클래스 1이 가질 샘플의 개수

**다운샘플링**은 클래스 레이블 1과 0을 서로 바꿔서 수행할 수 있다.
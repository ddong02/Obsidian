## 1. 선형변환의 개념과 함수
### 1) 선형변환의 정의

$V$와 $W$가 벡터공간이고 $\mathbf{u}, \mathbf{v}$가 $V$에 속하며 $\alpha$가 실수일 경우, $V$로부터 $W$로 가는 함수 $L$이 다음의 2가지 공리를 만족시킬 때 **선형변환(linear transformation)** 또는 **선형사상(linear mapping)**이라고 한다.

$$
L:V \rightarrow W
$$

1. $L(\mathbf{u} + \mathbf{v}) = L(\mathbf{u}) + L(\mathbf{v})$
2. $L(\alpha \mathbf{u}) = \alpha L (\mathbf{u})$

특히 $V = W$ 일 경우에는 선형변환 $L$을 $V$상에서의 **선형연산자(linear operator)**라고 한다.

**예제 9-1**
벡터공간 $\mathbf{R}^2$상의 벡터 $\mathbf{x}$에 대하여 $L$이 다음과 같이 정의된 함수일 때 선형변환인지의 여부를 살펴보자.

$$
L(\mathbf{x}) = 3 \mathbf{x}
$$

**풀이**

$$
\begin {align}
L(\boldsymbol{x} + \boldsymbol{y}) &= 3(\boldsymbol{x} + \boldsymbol{y})\\
&= 3 \boldsymbol{x} + 3 \boldsymbol{y} \\
&= L(\boldsymbol{x}) + L(\boldsymbol{y})\\ \\
L(\alpha \boldsymbol{x}) &= 3(\alpha \boldsymbol{x})\\
&= \alpha (3 \boldsymbol{x}) \\
&= \alpha L(\boldsymbol{x})
\end {align}
$$

선형변환의 2가지 조건을 만족하므로 $L$은 선형변환이다.

---

### 2) 함수와 선형변환

**함수의 개념**

집합 $X$에서 집합 $Y$로의 관계의 부분집합으로써, 집합 $X$에 있는 모든 원소 $x$가 집합 $Y$에 있는 원소 중 한 개와 관계가 있을 경우 $f$를 **함수(function)**라고 하며 다음과 같이 나타낸다.

$$
f : X \rightarrow Y
$$

여기서 $X$를 함수 $f$ 의 **정의역(domain)**이라고 하며, $Y$를 함수 $f$ 의 **공변역(codomain)**이라고 한다.

$f:X \rightarrow Y$ 를 함수라고 할 때 $x \in X$ 와 $y \in Y$ 에 대해 $(x, y) \in f$ 이면 $f(x) = y$ 라고 표시하며,
$y$ 를 함수 $f$ 에 의한 $x$ 의 **상(image)** 또는 **함수값**이라고 한다.

이 경우 $y$ 들의 집합을 **치역(range)**이라고 한다.

**선형변환**은 **함수의 조건을 만족**시키면서 다음의 **두 가지 조건**을 추가로 만족시켜야 한다.
1. $L(\mathbf{u}+\mathbf{v})=L(\mathbf{u}) + L(\mathbf{v})$
2. $L(\alpha \mathbf{u}) = \alpha L (\mathbf{u})$

→ 두 벡터의 합과 스칼라 곱에 대한 닫힌 성질

---

**변환**
입력 값과 출력 값이 모두 벡터인 함수를 **변환(transformation)**이라고 한다.

$L$ 이 벡터 $\mathbf{x}$로부터 벡터 $\mathbf{w}$로 보내는 변환일 경우 $L(\mathbf{x}) = \mathbf{w}$ 로 표현한다.

---

**커널**
$L : V \rightarrow W$ 가 벡터공간 $V$에서 $W$로의 선형변환이라고 할 때, $ker(L)$로 나타내는 $L$의 **커널(kernel)**은
$L(\mathbf{v}) = \mathbf{0}$ 를 만족하는 $V$의 부분집합 요소들이다.

![[chapter9.1-1.excalidraw.png|450]]

---

### 3) 여러 가지 선형변환

**사영변환**

- **예제 9-7**
$L:\mathbf{R}^3 \rightarrow \mathbf{R}^2$ 이 다음과 같이 정의될 때 $L$이 선형변환이 되는지를 살펴보자.

$$
L
\begin {pmatrix}
\begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix} 
\end {pmatrix} =
\begin {bmatrix} a_1 \\ a_2 \end {bmatrix} 
$$

- **풀이**

$\mathbf{u}$와 $\mathbf{v}$를 각각 다음과 같이 정의하자.

$$
\mathbf{u} =
\begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix}, \quad
\mathbf{v} =
\begin {bmatrix} b_1 \\ b_2 \\ b_3 \end {bmatrix}
$$

먼저 합에 관한 조건을 적용하면

$$
\begin {align}
L(\mathbf{u} + \mathbf{v}) &= L
\begin {pmatrix}
\begin {bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end {bmatrix}
\end {pmatrix} \\ \\ &=
\begin {bmatrix} a_1 + b_1 \\ a_2 + b_2 \end {bmatrix}
= \begin {bmatrix} a_1 \\ a_2 \end {bmatrix} +
\begin {bmatrix} b_1 \\ b_2 \end {bmatrix}
= L(\mathbf{u}) + L(\mathbf{v})
\end {align}
$$
가 성립한다.

또한 $\alpha$가 실수라면

$$
L(\alpha \mathbf{u}) = L
\begin {pmatrix}
\begin {bmatrix}
\alpha a_1 \\
\alpha a_2 \\
\alpha a_3
\end {bmatrix}
\end {pmatrix} =
\begin {bmatrix}
\alpha a_1 \\
\alpha a_2
\end {bmatrix} =
\alpha
\begin {bmatrix} a_1 \\ a_2 \end {bmatrix} =
\alpha L(\mathbf{u})
$$


가 성립한다. 따라서 $L$은 선형변환이다.

특히 이런 경우에는 **사영변환(projection transformation)**이라고 하는데, $\mathbf{R}^3$상의 벡터
$\mathbf{u}$를 $\mathbf{R}^2$ 인 $x$-$y$ 평면에 수직으로 사영한 변환이다.

---

**확대변환과 축소변환**

- **예제 9-8**
$L:\mathbf{R}^3 \rightarrow \mathbf{R}^3$이 다음과 같이 정의되고, $r$이 실수일 때 $L$이 선형변환이 되는지를 살펴보자.

$$
L =
\left(
\begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix}
\right) =
r
\begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix}
$$

- **풀이**

$\mathbf{u}$와 $\mathbf{v}$를 각각 다음과 같이 정의하자.

$$
\mathbf{u} =
\begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix}, \quad
\mathbf{v} =
\begin {bmatrix} b_1 \\ b_2 \\ b_3 \end {bmatrix}
$$

먼저 합에 관한 조건에 따라 전개하면

$$
\begin {align}
L(\mathbf{u} + \mathbf{v}) &= L
\left (
\begin {bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end {bmatrix}
\right ) \\ \\ &=
r
\begin {bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end {bmatrix} \\ \\ &=
r \begin {bmatrix} a_1 \\ a_2 \\ a_3 \end {bmatrix} +
r \begin {bmatrix} b_1 \\ b_2 \\ b_3 \end {bmatrix} \\ \\ &=
L(\mathbf{u}) + L(\mathbf{v})
\end{align}
$$

$\alpha$가 실수라면

$$
L(\alpha \mathbf{u}) = L
\left (
\begin {bmatrix}
\alpha a_1 \\
\alpha a_2 \\
\alpha a_3
\end {bmatrix}
\right ) =
r
\begin {bmatrix}
\alpha a_1 \\
\alpha a_2 \\
\alpha a_3
\end {bmatrix} =
r \alpha
\begin {bmatrix}
a_1 \\
a_2 \\
a_3 
\end {bmatrix} =
\alpha L(\mathbf{u})
$$

가 성립한다. 따라서 $L$은 선형변환이다.

$L : \mathbf{R}^3 \rightarrow \mathbf{R}^3$ 인 경우에는 둘 다 3차원상의 선형변환이므로 $L$은 **선형연산자**가 된다.

만약 $r > 1$ 일 경우에는 $L$을 **확대변환(dilation transformation)**이라 하고,
$0 < r < 1$ 인 경우에는 $L$을 **축소변환(contraction transformation)**이라고 한다.

---

**반사변환**

- **예제 9-9**
$L : \mathbf{R}^2 \rightarrow \mathbf{R}^2$ 이 다음과 같이 정의될 때 $L$이 선형변환이 되는지를 살펴보자.

$$
L
\left (
\begin {bmatrix} a_1 \\ a_2 \end {bmatrix}
\right ) =
\begin {bmatrix} a_1 \\ -a_2 \end {bmatrix}
$$

- **풀이**

$\mathbf{u}$와 $\mathbf{v}$를 각각 다음과 같이 정의하자.

$$
\mathbf{u} =
\begin {bmatrix} a_1 \\ a_2 \end {bmatrix}, \quad
\mathbf{v} =
\begin {bmatrix} b_1 \\ b_2 \end {bmatrix}
$$

먼저 합에 관한 조건에 따라 전개하면

$$
\begin {align}
L(\mathbf{u} + \mathbf{v}) &= L
\left (
\begin {bmatrix} a_1 + b_1 \\ a_2 + b_2 \end {bmatrix}
\right ) \\\\  &=
\begin {bmatrix} a_1 + b_1 \\ -(a_2 + b_2) \end {bmatrix} \\\\ &=
\begin {bmatrix} a_1 + b_1 \\ -a_2 -b_2 \end {bmatrix} \\\\ &=
\begin {bmatrix} a_1 \\ -a_2\end {bmatrix} +
\begin {bmatrix} b_1 \\ -b_2\end {bmatrix} \\\\ &=
L(\mathbf{u}) + L(\mathbf{v})
\end {align}
$$


가 성립한다.

$\alpha$가 실수라면

$$
L(\alpha \mathbf{u}) = L
\left (
\begin {bmatrix} \alpha a_1 \\ \alpha a_2 \end {bmatrix}
\right )
=
\begin {bmatrix} \alpha a_1 \\ -\alpha a_2 \end {bmatrix} = \alpha
\begin {bmatrix} a_1 \\ -a_2 \end {bmatrix} =
\alpha L(\mathbf{u})
$$

가 성립한다. 따라서 $L$은 선형변환이다.

$L \left ( \begin {bmatrix} a_1 \\ a_2 \end {bmatrix} \right ) = \begin {bmatrix} a_1 \\ -a_2 \end {bmatrix}$ 은 $x$축에 대한 **반사변환(reflection transformation)**이다.

만약 $L$이 다음과 같다면 $L \left ( \begin {bmatrix} a_1 \\ a_2 \end {bmatrix} \right ) = \begin {bmatrix} -a_1 \\ a_2 \end {bmatrix}$ 은 $y$축에 대한 **반사변환**이다.

---

**회전변환**

선형변환 $L : \mathbf{R}^2 \rightarrow \mathbf{R}^2$이 다음과 같이 정의될 때

$$
L
\left (
\begin {bmatrix} x \\ y \end {bmatrix}
\right ) =
\begin {bmatrix} \cos \phi & -\sin \phi \\ \sin \phi & \cos \phi \end {bmatrix}
\begin {bmatrix} x \\ y \end {bmatrix}
$$

만약 $\mathbf{v} = \begin {bmatrix} x \\ y \end {bmatrix}$ 라고 하면 다음과 같이 변환된다.

$$
L(\mathbf{v}) =
\begin {bmatrix}
x\cos \phi -y \sin \phi \\
x \sin \phi + y \cos \phi
\end {bmatrix}
$$

이 경우 $L$을 **회전변환(rotation transformation)**이라고 하는데, $L(\mathbf{v})$는 원래의 벡터가 나타내는 점$P(x, y)$ 로부터 원점 $O$를 중심축으로 $\phi$ 각도만큼 $P^\prime = (x^{\prime}, y^{\prime})$ 로 회전변환된 벡터이다.

![[chapter9.1-2.excalidraw.png|350]]

---

### 4) 변환의 표준행렬

•••